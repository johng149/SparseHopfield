{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "exlq46O9zfGw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import einops\n",
        "from torch.nn import Module, Parameter, ParameterList\n",
        "from torch import Tensor\n",
        "from collections import namedtuple\n",
        "from typing import List\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsEMqglEiKCR",
        "outputId": "8985aa58-aee0-4de1-f844-70591a52c409"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f2c3fff7d50>"
            ]
          },
          "execution_count": 113,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.manual_seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "uuv7HyJC1oKo"
      },
      "outputs": [],
      "source": [
        "LayerInfo = namedtuple(\"LayerInfo\", [\"nodes\", \"memories\"]) # num nodes, num mems per node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "EQNAHQdQzqay"
      },
      "outputs": [],
      "source": [
        "class SparseHopfield(Module):\n",
        "  def __init__(self, receptive_fields, field_dim, layers: List[LayerInfo], t: int = 0, alpha: float=1.0, rho=1e-8):\n",
        "    super().__init__()\n",
        "    self.fields = receptive_fields\n",
        "    self.field_dim = field_dim\n",
        "    self.layers_info = layers\n",
        "    self.t = t\n",
        "    self.a = alpha\n",
        "    self.rho = rho # used to prevent division by zero\n",
        "    self.kwargs = {\n",
        "        \"receptive_fields\": self.fields,\n",
        "        \"field_dim\": self.field_dim,\n",
        "        \"layers\": self.layers_info,\n",
        "        \"t\": self.t,\n",
        "        \"alpha\": self.a,\n",
        "        \"rho\": self.rho\n",
        "    }\n",
        "\n",
        "    self.growth_threshold = self.a / (self.t + self.a)\n",
        "    self.layers = []\n",
        "    self.layer_counts = []\n",
        "\n",
        "    self.create_layers()\n",
        "\n",
        "  def reset_iteration(self, t: int = 0):\n",
        "    self.t = t\n",
        "    self.growth_threshold = self.a / (self.t + self.a)\n",
        "\n",
        "  def update_iteration(self):\n",
        "    self.t += 1\n",
        "    self.growth_threshold = self.a / (self.t + self.a)\n",
        "\n",
        "  def create_layers(self):\n",
        "    for i in range(len(self.layers_info)):\n",
        "      if i == 0: # assume we are in outer layer\n",
        "        nodes, mems = self.layers_info[i]\n",
        "        nodes = self.fields # we ignore what layers info says, we need to match the fields here\n",
        "        self.layers_info[i] = LayerInfo(nodes=nodes, memories=mems)\n",
        "        dim = self.field_dim\n",
        "        counts = torch.ones(nodes, mems)\n",
        "        mem_matrix = Parameter(torch.zeros((nodes, mems, dim)) + 0.5, requires_grad=False)\n",
        "        self.layers.append(mem_matrix)\n",
        "        self.layer_counts.append(counts)\n",
        "      else:\n",
        "        nodes, mems = self.layers_info[i]\n",
        "        prev_nodes, prev_mems = self.layers_info[i - 1]\n",
        "        assert prev_nodes % nodes == 0, f\"At layer {i}, prev is {prev_nodes} but nodes is {nodes}\"\n",
        "        children_per_node = prev_nodes // nodes\n",
        "        counts = torch.ones(nodes, mems)\n",
        "        mem_matrix = Parameter(torch.zeros((nodes, children_per_node, mems, prev_mems)), requires_grad=False)\n",
        "        self.layers.append(mem_matrix)\n",
        "        self.layer_counts.append(counts)\n",
        "      self.layers = ParameterList(self.layers)\n",
        "\n",
        "  @staticmethod\n",
        "  def batch_min(tensor, batch_size: int):\n",
        "      sorts, sorti = torch.sort(tensor)\n",
        "      nodes, memes = sorti.shape\n",
        "      full_expands = (batch_size // memes) + 1\n",
        "      partial_expands = batch_size % memes\n",
        "      sorti_expanded = sorti.unsqueeze(1).expand(-1, full_expands, -1).reshape(nodes, -1)\n",
        "      sorti_selection = sorti_expanded[:, :(full_expands-1)*memes + partial_expands]\n",
        "      return sorti_selection, sorti_selection.T.reshape(batch_size, -1, 1)\n",
        "\n",
        "  @staticmethod\n",
        "  def maxi(x):\n",
        "    max_indices = torch.argmax(x, dim=-1, keepdim=True)\n",
        "    blank = torch.zeros_like(x)\n",
        "    return torch.scatter(blank, -1, max_indices, torch.gather(x, -1, max_indices))\n",
        "\n",
        "  @staticmethod\n",
        "  def argmaxi(x, eps=1e-8):\n",
        "    maxied = torch.abs(SparseHopfield.maxi(x))\n",
        "    factors, indices = torch.max(maxied, dim=-1)\n",
        "    factors = factors - eps\n",
        "    return maxied / factors.unsqueeze(-1)\n",
        "\n",
        "  @staticmethod\n",
        "  def growth_argmaxi(x, counts, eps=1e-8, threshold=0.9):\n",
        "    batch_size, nodes, mems = x.shape\n",
        "    normal_path = SparseHopfield.argmaxi(x, eps)\n",
        "    indices, indices_sg = SparseHopfield.batch_min(counts, batch_size)\n",
        "    # indices_sg = indices.unsqueeze(0).expand(batch_size,-1,-1)\n",
        "    growth_path = torch.zeros_like(normal_path)\n",
        "    # values_of_interest = torch.gather(x, -1, indices_sg)\n",
        "    # taking a closer look, it doesn't seem like the exact values from\n",
        "    # the original should matter, we were just using it to divide\n",
        "    # by itself (minus an eps) in the argmaxi to turn them into 1s,\n",
        "    # to just setting it to 1 should be fine?\n",
        "    values_of_interest = 1\n",
        "    growth_path = torch.scatter(growth_path, -1, indices_sg, values_of_interest)\n",
        "    trigger_growth = torch.sum(x > threshold, dim=-1, keepdim=True) <= 0\n",
        "    grown = torch.where(trigger_growth, growth_path, normal_path)\n",
        "\n",
        "    #count_needs_update = einops.reduce(trigger_growth, 'batch nodes flag -> nodes', 'sum')\n",
        "    count_values_of_interest = torch.gather(counts, -1, indices).T.reshape(batch_size, -1, 1)\n",
        "    res = torch.where(trigger_growth, 0, count_values_of_interest)\n",
        "    batched_counts = counts.unsqueeze(0).expand(batch_size, -1, -1)\n",
        "    updated_batch_counts = torch.scatter(batched_counts, -1, indices_sg, res)\n",
        "    updated_counts = einops.reduce(updated_batch_counts, 'batch nodes mems -> nodes mems', 'min')\n",
        "    updated_counts = updated_counts + einops.reduce(grown, 'batch nodes mems -> nodes mems', 'sum')\n",
        "\n",
        "    return grown, updated_counts.detach()\n",
        "\n",
        "  @staticmethod\n",
        "  def outer_forward_parallel(mems, xs, rho=1e-8):\n",
        "    batch_size, fields, dim = xs.shape\n",
        "\n",
        "    # if mems only has 3 dimensions, then we know it is missing the batch dim,\n",
        "    # this is because the shape of mems should be\n",
        "    # batch_size x fields x memories x dim\n",
        "    m = mems - 0.5 if mems.dim() == 4 else (mems - 0.5).expand(batch_size, -1, -1, -1)\n",
        "    x = xs - 0.5\n",
        "    numerator = einops.einsum(m,x, 'batch fields memories dim, batch fields dim -> batch fields memories') * 0.5\n",
        "    m_norm = torch.sqrt(einops.reduce(m ** 2, 'batch fields memories dim -> batch fields memories', 'sum'))\n",
        "    x_norm = torch.sqrt(einops.reduce(x ** 2, 'batch field dim -> batch field', 'sum')).unsqueeze(-1)\n",
        "    denom = m_norm * x_norm + rho\n",
        "    return (numerator / denom) + 0.5\n",
        "\n",
        "  @staticmethod\n",
        "  def hidden_forward_parallel(hidden_mm, children_x, rho=1e-8):\n",
        "    batch_size, total_children, children_mem_cols = children_x.shape\n",
        "    if hidden_mm.dim() == 4:\n",
        "      num_hidden_nodes, children_per_hidden, hidden_mems, children_mem_cols = hidden_mm.shape\n",
        "      mm = hidden_mm.expand(batch_size, -1, -1, -1, -1)\n",
        "    else:\n",
        "      batch_size, num_hidden_nodes, children_per_hidden, hidden_mems, children_mem_cols = hidden_mm.shape\n",
        "      mm = hidden_mm\n",
        "    x = SparseHopfield.maxi(children_x).reshape(batch_size, num_hidden_nodes, children_per_hidden, children_mem_cols)\n",
        "    propagation = einops.einsum(mm, x, 'batch hidden children h_mems c_mems, batch hidden children c_mems -> batch hidden h_mems')\n",
        "    x_norm = torch.sqrt(einops.reduce(x**2,'batch hidden children c_mems -> batch hidden', 'sum')).unsqueeze(-1)\n",
        "    norm_coeff = 1 / ((children_per_hidden * x_norm) + rho)\n",
        "    return propagation * norm_coeff\n",
        "\n",
        "  @staticmethod\n",
        "  def down_prop_parallel(parent_h, parent_mm, child_h, coeff=0.5):\n",
        "    # we assume `parent_h` has already been passed through argmaxi\n",
        "    # in other words down_prop_parallel(argmaxi(parent_h), ...)\n",
        "    batch_size, children, children_dim = child_h.shape\n",
        "    if parent_mm.dim() == 4:\n",
        "      mm = parent_mm.expand(batch_size, -1, -1, -1, -1)\n",
        "    else:\n",
        "      mm = parent_mm\n",
        "    batch_size, parent_nodes, children_per_parent, parent_dim, child_dim = mm.shape\n",
        "    argmaxi_parent_h = parent_h.unsqueeze(-2).expand(-1,-1, children_per_parent, -1)\n",
        "    orig = child_h * coeff\n",
        "    new = (1 - coeff) * einops.einsum(argmaxi_parent_h, mm, 'batch parents children pdim, batch parents children pdim cdim -> batch parents children cdim')\n",
        "    new = new.reshape(batch_size, children, -1)\n",
        "    return orig + new\n",
        "\n",
        "  @staticmethod\n",
        "  def pred(parent_down_prop, parent_mem_matrix):\n",
        "    mm = parent_mem_matrix if parent_mem_matrix.dim() != 3 else parent_mem_matrix.unsqueeze(-3)\n",
        "    nodes, children_per_node, memories, dim = mm.shape\n",
        "    batch_size, nodes, memories = parent_down_prop.shape\n",
        "    prediction = einops.einsum(mm, parent_down_prop, 'nodes children_per_node memories dim, batch nodes memories -> batch nodes children_per_node dim')\n",
        "    prediction = prediction.reshape(batch_size, nodes * children_per_node, dim)\n",
        "    return prediction\n",
        "\n",
        "  @staticmethod\n",
        "  def mem_delta(parent_down_prop, parent_mem_matrix, child_down_prop):\n",
        "    \"\"\"\n",
        "    Calculates the delta that should be added to the given `parent_mem_matrix`\n",
        "    to optimize the model. For example, to optimize the given matrix, use\n",
        "    `new_mm = old_mm + lr * mem_delta(p_prop, old_mm, c_prop)`\n",
        "    Note that the `lr` scaling should be done outside of this function\n",
        "\n",
        "    Note also that, according to the paper / reference implementation, when\n",
        "    calculating the matrix update for the outer layer, the child down prop is\n",
        "    just the raw inputs, not one that is one-hot encoded\n",
        "    \"\"\"\n",
        "    # shape is nodes memories dim, but we want\n",
        "    # nodes children_per_node memories dim\n",
        "    # in this case, we'll assume that children_per_node is 1\n",
        "    mm = parent_mem_matrix if parent_mem_matrix.dim() != 3 else parent_mem_matrix.unsqueeze(-3)\n",
        "    nodes, children_per_node, memories, dim = mm.shape\n",
        "    batch_size, nodes, memories = parent_down_prop.shape\n",
        "\n",
        "    prediction = SparseHopfield.pred(parent_down_prop, mm)\n",
        "    error = child_down_prop - prediction\n",
        "    error = error.reshape(batch_size, nodes, children_per_node, dim)\n",
        "    delta = einops.einsum(error, parent_down_prop, 'batch nodes children_per_node dim, batch nodes memories -> nodes children_per_node memories dim')\n",
        "    return delta.reshape(parent_mem_matrix.shape)\n",
        "\n",
        "  def outer_up(self, sensor_input):\n",
        "    outer_mm = self.layers[0]\n",
        "    h_sub_l = SparseHopfield.outer_forward_parallel(outer_mm, sensor_input, self.rho)\n",
        "    return h_sub_l\n",
        "\n",
        "  def up(self, sensor_input):\n",
        "    upwards = [self.outer_up(sensor_input)]\n",
        "    for i in range(1, len(self.layers)):\n",
        "      mm = self.layers[i]\n",
        "      h_sub_l = SparseHopfield.hidden_forward_parallel(mm, upwards[i-1], self.rho)\n",
        "      upwards.append(h_sub_l)\n",
        "    return upwards\n",
        "\n",
        "  def root_down(self, upwards, eps=1e-6):\n",
        "    root_h_sub_l = upwards[-1]\n",
        "    root_counts = self.layer_counts[-1]\n",
        "    root_h_sub_l_star, root_counts = SparseHopfield.growth_argmaxi(root_h_sub_l, root_counts, eps, self.growth_threshold)\n",
        "    self.layer_counts[-1] = root_counts\n",
        "    return root_h_sub_l_star\n",
        "\n",
        "  def down(self, upwards, eps=1e-6, coeff=0.5):\n",
        "    downwards = [self.root_down(upwards, eps)]\n",
        "    for i in range(len(upwards)-2, -1, -1):\n",
        "      h_sub_l = upwards[i]\n",
        "      counts = self.layer_counts[i]\n",
        "      downed = SparseHopfield.down_prop_parallel(downwards[-1],self.layers[i+1],h_sub_l,coeff)\n",
        "      h_sub_l_star, counts = SparseHopfield.growth_argmaxi(downed, counts, eps, self.growth_threshold)\n",
        "      self.layer_counts[i] = counts\n",
        "      downwards.append(h_sub_l_star)\n",
        "    return downwards\n",
        "\n",
        "  def pred_root_down(self, upwards, eps=1e-6):\n",
        "    root_h_sub_l = upwards[-1]\n",
        "    root_counts = self.layer_counts[-1]\n",
        "    root_h_sub_l_star = SparseHopfield.argmaxi(root_h_sub_l, eps)\n",
        "    return root_h_sub_l_star\n",
        "\n",
        "  def pred_down(self, upwards, eps=1e-6, coeff=0.5):\n",
        "    downwards = [self.pred_root_down(upwards, eps)]\n",
        "    for i in range(len(upwards)-2, -1, -1):\n",
        "      h_sub_l = upwards[i]\n",
        "      counts = self.layer_counts[i]\n",
        "      downed = SparseHopfield.down_prop_parallel(downwards[-1],self.layers[i+1],h_sub_l,coeff)\n",
        "      h_sub_l_star = SparseHopfield.argmaxi(downed, eps)\n",
        "      downwards.append(h_sub_l_star)\n",
        "    return downwards\n",
        "\n",
        "  def delta_outer(self, downwards, sensory_input):\n",
        "    outer_mm = self.layers[0]\n",
        "    outer_h_sub_l_star = downwards[-1]\n",
        "    outer_delta = SparseHopfield.mem_delta(outer_h_sub_l_star, outer_mm, sensory_input)\n",
        "    return outer_delta\n",
        "\n",
        "  def delta(self, downwards, sensory_input):\n",
        "    deltas = [self.delta_outer(downwards, sensory_input)]\n",
        "    for i in range(1, len(downwards)):\n",
        "      child_h_sub_l_star = downwards[len(downwards) - i]\n",
        "      h_sub_l_star = downwards[len(downwards) - i - 1]\n",
        "      mm = self.layers[i]\n",
        "      delta = SparseHopfield.mem_delta(h_sub_l_star, mm, child_h_sub_l_star)\n",
        "      deltas.append(delta)\n",
        "    return deltas\n",
        "\n",
        "  def optim_outer(self, deltas):\n",
        "    outer_delta = deltas[0]\n",
        "    outer_mm = self.layers[0]\n",
        "    outer_counts = self.layer_counts[0]\n",
        "    delta = outer_delta / (outer_counts).unsqueeze(-1)\n",
        "    self.layers[0] = outer_mm + delta\n",
        "\n",
        "  def optim(self, deltas):\n",
        "    self.optim_outer(deltas)\n",
        "    for i in range(1, len(deltas)):\n",
        "      delta = deltas[i]\n",
        "      mm = self.layers[i]\n",
        "      count = self.layer_counts[i]\n",
        "      delta = delta / (count).unsqueeze(-1).unsqueeze(-3)\n",
        "      self.layers[i] = mm + delta\n",
        "    self.update_iteration()\n",
        "\n",
        "  def optimize(self, sensory_input, eps=1e-6, coeff=0.5):\n",
        "    upwards = self.up(sensory_input)\n",
        "    downwards = self.down(upwards, eps, coeff)\n",
        "    deltas = self.delta(downwards, sensory_input)\n",
        "    self.optim(deltas)\n",
        "\n",
        "  def predict(self, sensory_input, eps=1e-6, coeff=0.5):\n",
        "    with torch.no_grad():\n",
        "      upwards = self.up(sensory_input)\n",
        "      downwards = self.pred_down(upwards, eps, coeff)\n",
        "      prediction = SparseHopfield.pred(downwards[-1],self.layers[0])\n",
        "      return prediction\n",
        "\n",
        "  def prediction_error(self, sensory_input, eps=1e-6, coeff=0.5):\n",
        "    with torch.no_grad():\n",
        "      pred = self.predict(sensory_input, eps, coeff)\n",
        "      return torch.mean(torch.square(pred - sensory_input))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "Lp9Se0aP9gy9"
      },
      "outputs": [],
      "source": [
        "layers: List[LayerInfo] = [\n",
        "    LayerInfo(nodes=-1, memories=15),\n",
        "    LayerInfo(nodes=3, memories=15),\n",
        "    LayerInfo(nodes=1, memories=15)\n",
        "]\n",
        "receptive_fields = 6\n",
        "field_dim = 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(net, data):\n",
        "    batch_size, fields, dim = data.shape\n",
        "    for i in range(batch_size):\n",
        "        net.optimize(data[i].unsqueeze(0))\n",
        "\n",
        "def loss(net, data):\n",
        "    batch_size, fields, dim = data.shape\n",
        "    total_loss = 0\n",
        "    errors = []\n",
        "    for i in range(batch_size):\n",
        "        err = net.prediction_error(data[i].unsqueeze(0))\n",
        "        errors.append(err)\n",
        "        total_loss += err\n",
        "    return total_loss / batch_size, errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "CPqSlqm69kaT"
      },
      "outputs": [],
      "source": [
        "net = SparseHopfield(receptive_fields=receptive_fields,field_dim=field_dim,layers=layers, alpha=3.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "x7HuJ9xRU9Mn"
      },
      "outputs": [],
      "source": [
        "batch_size = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "zzfV-UkxcZcI"
      },
      "outputs": [],
      "source": [
        "sensory_input = torch.randn(batch_size, receptive_fields, field_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "Edjxb9REm8kw"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor(1.2568), [tensor(1.1588), tensor(1.3570), tensor(1.2545)])"
            ]
          },
          "execution_count": 121,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loss(net, sensory_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor(0.6939), [tensor(0.8913), tensor(0.6124), tensor(0.5779)])"
            ]
          },
          "execution_count": 122,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train(net, sensory_input)\n",
        "loss(net, sensory_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
