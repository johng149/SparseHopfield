{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 230,
      "metadata": {
        "id": "exlq46O9zfGw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import einops\n",
        "from torch.nn import Module, Parameter, ParameterList\n",
        "from torch import Tensor\n",
        "from collections import namedtuple\n",
        "from typing import List\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 231,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsEMqglEiKCR",
        "outputId": "8985aa58-aee0-4de1-f844-70591a52c409"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7e585c23fbf0>"
            ]
          },
          "execution_count": 231,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.manual_seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 232,
      "metadata": {
        "id": "uuv7HyJC1oKo"
      },
      "outputs": [],
      "source": [
        "LayerInfo = namedtuple(\"LayerInfo\", [\"nodes\", \"memories\"]) # num nodes, num mems per node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 233,
      "metadata": {
        "id": "EQNAHQdQzqay"
      },
      "outputs": [],
      "source": [
        "class SparseHopfield(Module):\n",
        "  def __init__(self, receptive_fields, field_dim, layers: List[LayerInfo], t: int = 0, alpha: float=1.0, rho=1e-8):\n",
        "    super().__init__()\n",
        "    self.fields = receptive_fields\n",
        "    self.field_dim = field_dim\n",
        "    self.layers_info = layers\n",
        "    self.t = t\n",
        "    self.a = alpha\n",
        "    self.rho = rho # used to prevent division by zero\n",
        "    self.kwargs = {\n",
        "        \"receptive_fields\": self.fields,\n",
        "        \"field_dim\": self.field_dim,\n",
        "        \"layers\": self.layers_info,\n",
        "        \"t\": self.t,\n",
        "        \"alpha\": self.a,\n",
        "        \"rho\": self.rho\n",
        "    }\n",
        "\n",
        "    self.growth_threshold = self.a / (self.t + self.a)\n",
        "    self.layers = []\n",
        "    self.layer_counts = []\n",
        "\n",
        "    self.create_layers()\n",
        "\n",
        "  def reset_iteration(self, t: int = 0):\n",
        "    self.t = t\n",
        "    self.growth_threshold = self.a / (self.t + self.a)\n",
        "\n",
        "  def update_iteration(self):\n",
        "    self.t += 1\n",
        "    self.growth_threshold = self.a / (self.t + self.a)\n",
        "\n",
        "  def create_layers(self):\n",
        "    for i in range(len(self.layers_info)):\n",
        "      if i == 0: # assume we are in outer layer\n",
        "        nodes, mems = self.layers_info[i]\n",
        "        nodes = self.fields # we ignore what layers info says, we need to match the fields here\n",
        "        self.layers_info[i] = LayerInfo(nodes=nodes, memories=mems)\n",
        "        dim = self.field_dim\n",
        "        counts = torch.ones(nodes, mems)\n",
        "        mem_matrix = Parameter(torch.zeros((nodes, mems, dim)) + 0.5, requires_grad=False)\n",
        "        self.layers.append(mem_matrix)\n",
        "        self.layer_counts.append(counts)\n",
        "      else:\n",
        "        nodes, mems = self.layers_info[i]\n",
        "        prev_nodes, prev_mems = self.layers_info[i - 1]\n",
        "        assert prev_nodes % nodes == 0, f\"At layer {i}, prev is {prev_nodes} but nodes is {nodes}\"\n",
        "        children_per_node = prev_nodes // nodes\n",
        "        counts = torch.ones(nodes, mems)\n",
        "        mem_matrix = Parameter(torch.zeros((nodes, children_per_node, mems, prev_mems)), requires_grad=False)\n",
        "        self.layers.append(mem_matrix)\n",
        "        self.layer_counts.append(counts)\n",
        "      self.layers = ParameterList(self.layers)\n",
        "\n",
        "  @staticmethod\n",
        "  def maxi(x):\n",
        "    max_indices = torch.argmax(x, dim=-1, keepdim=True)\n",
        "    blank = torch.zeros_like(x)\n",
        "    return torch.scatter(blank, -1, max_indices, torch.gather(x, -1, max_indices))\n",
        "\n",
        "  @staticmethod\n",
        "  def argmaxi(x, eps=1e-8):\n",
        "    maxied = torch.abs(SparseHopfield.maxi(x))\n",
        "    factors, indices = torch.max(maxied, dim=-1)\n",
        "    factors = factors - eps\n",
        "    return maxied / factors.unsqueeze(-1)\n",
        "\n",
        "  @staticmethod\n",
        "  def growth_argmaxi(x, counts, eps=1e-8, threshold=0.9):\n",
        "    # for now we assume batch_size is 1\n",
        "    batch_size, nodes, mems = x.shape\n",
        "    normal_path = SparseHopfield.argmaxi(x, eps)\n",
        "    _, indices = torch.min(counts, dim=-1, keepdim=True)\n",
        "    indices_sg = indices.unsqueeze(0).expand(batch_size,-1,-1)\n",
        "    growth_path = torch.zeros_like(normal_path)\n",
        "    # values_of_interest = torch.gather(x, -1, indices_sg)\n",
        "    # taking a closer look, it doesn't seem like the exact values from\n",
        "    # the original should matter, we were just using it to divide\n",
        "    # by itself (minus an eps) in the argmaxi to turn them into 1s,\n",
        "    # to just setting it to 1 should be fine?\n",
        "    values_of_interest = 1\n",
        "    growth_path = torch.scatter(growth_path, -1, indices_sg, values_of_interest)\n",
        "    growth_path = SparseHopfield.argmaxi(torch.abs(growth_path), eps)\n",
        "    trigger_growth = torch.sum(x > threshold, dim=-1, keepdim=True) <= 0\n",
        "    grown = torch.where(trigger_growth, growth_path, normal_path)\n",
        "\n",
        "    _, activated_indices = torch.max(grown, dim=-1, keepdim=True)\n",
        "    selected_counts = torch.gather(counts.unsqueeze(0).expand(batch_size, -1, -1), -1, activated_indices)\n",
        "    zeroed_out_counts = selected_counts * ~trigger_growth\n",
        "    updated_counts = torch.scatter(counts.unsqueeze(0).expand(batch_size, -1, -1), -1, activated_indices, zeroed_out_counts)\n",
        "    updated_counts = updated_counts + grown\n",
        "    updated_counts = einops.reduce(updated_counts, 'batch nodes mems -> nodes mems', 'sum')\n",
        "\n",
        "    return grown, updated_counts.detach()\n",
        "\n",
        "  @staticmethod\n",
        "  def outer_forward_parallel(mems, xs, rho=1e-8):\n",
        "    batch_size, fields, dim = xs.shape\n",
        "\n",
        "    # if mems only has 3 dimensions, then we know it is missing the batch dim,\n",
        "    # this is because the shape of mems should be\n",
        "    # batch_size x fields x memories x dim\n",
        "    m = mems - 0.5 if mems.dim() == 4 else (mems - 0.5).expand(batch_size, -1, -1, -1)\n",
        "    x = xs - 0.5\n",
        "    numerator = einops.einsum(m,x, 'batch fields memories dim, batch fields dim -> batch fields memories') * 0.5\n",
        "    m_norm = torch.sqrt(einops.reduce(m ** 2, 'batch fields memories dim -> batch fields memories', 'sum'))\n",
        "    x_norm = torch.sqrt(einops.reduce(x ** 2, 'batch field dim -> batch field', 'sum')).unsqueeze(-1)\n",
        "    denom = m_norm * x_norm + rho\n",
        "    return (numerator / denom) + 0.5\n",
        "\n",
        "  @staticmethod\n",
        "  def hidden_forward_parallel(hidden_mm, children_x, rho=1e-8):\n",
        "    batch_size, total_children, children_mem_cols = children_x.shape\n",
        "    if hidden_mm.dim() == 4:\n",
        "      num_hidden_nodes, children_per_hidden, hidden_mems, children_mem_cols = hidden_mm.shape\n",
        "      mm = hidden_mm.expand(batch_size, -1, -1, -1, -1)\n",
        "    else:\n",
        "      batch_size, num_hidden_nodes, children_per_hidden, hidden_mems, children_mem_cols = hidden_mm.shape\n",
        "      mm = hidden_mm\n",
        "    x = SparseHopfield.maxi(children_x).reshape(batch_size, num_hidden_nodes, children_per_hidden, children_mem_cols)\n",
        "    propagation = einops.einsum(mm, x, 'batch hidden children h_mems c_mems, batch hidden children c_mems -> batch hidden h_mems')\n",
        "    x_norm = torch.sqrt(einops.reduce(x**2,'batch hidden children c_mems -> batch hidden', 'sum')).unsqueeze(-1)\n",
        "    norm_coeff = 1 / ((children_per_hidden * x_norm) + rho)\n",
        "    return propagation * norm_coeff\n",
        "\n",
        "  @staticmethod\n",
        "  def down_prop_parallel(parent_h, parent_mm, child_h, coeff=0.5):\n",
        "    # we assume `parent_h` has already been passed through argmaxi\n",
        "    # in other words down_prop_parallel(argmaxi(parent_h), ...)\n",
        "    batch_size, children, children_dim = child_h.shape\n",
        "    if parent_mm.dim() == 4:\n",
        "      mm = parent_mm.expand(batch_size, -1, -1, -1, -1)\n",
        "    else:\n",
        "      mm = parent_mm\n",
        "    batch_size, parent_nodes, children_per_parent, parent_dim, child_dim = mm.shape\n",
        "    argmaxi_parent_h = parent_h.unsqueeze(-2).expand(-1,-1, children_per_parent, -1)\n",
        "    orig = child_h * coeff\n",
        "    new = (1 - coeff) * einops.einsum(argmaxi_parent_h, mm, 'batch parents children pdim, batch parents children pdim cdim -> batch parents children cdim')\n",
        "    new = new.reshape(batch_size, children, -1)\n",
        "    return orig + new\n",
        "\n",
        "  @staticmethod\n",
        "  def pred(parent_down_prop, parent_mem_matrix):\n",
        "    mm = parent_mem_matrix if parent_mem_matrix.dim() != 3 else parent_mem_matrix.unsqueeze(-3)\n",
        "    nodes, children_per_node, memories, dim = mm.shape\n",
        "    batch_size, nodes, memories = parent_down_prop.shape\n",
        "    prediction = einops.einsum(mm, parent_down_prop, 'nodes children_per_node memories dim, batch nodes memories -> batch nodes children_per_node dim')\n",
        "    prediction = prediction.reshape(batch_size, nodes * children_per_node, dim)\n",
        "    return prediction\n",
        "\n",
        "  @staticmethod\n",
        "  def mem_delta(parent_down_prop, parent_mem_matrix, child_down_prop):\n",
        "    \"\"\"\n",
        "    Calculates the delta that should be added to the given `parent_mem_matrix`\n",
        "    to optimize the model. For example, to optimize the given matrix, use\n",
        "    `new_mm = old_mm + lr * mem_delta(p_prop, old_mm, c_prop)`\n",
        "    Note that the `lr` scaling should be done outside of this function\n",
        "\n",
        "    Note also that, according to the paper / reference implementation, when\n",
        "    calculating the matrix update for the outer layer, the child down prop is\n",
        "    just the raw inputs, not one that is one-hot encoded\n",
        "    \"\"\"\n",
        "    # shape is nodes memories dim, but we want\n",
        "    # nodes children_per_node memories dim\n",
        "    # in this case, we'll assume that children_per_node is 1\n",
        "    mm = parent_mem_matrix if parent_mem_matrix.dim() != 3 else parent_mem_matrix.unsqueeze(-3)\n",
        "    nodes, children_per_node, memories, dim = mm.shape\n",
        "    batch_size, nodes, memories = parent_down_prop.shape\n",
        "\n",
        "    prediction = SparseHopfield.pred(parent_down_prop, mm)\n",
        "    error = child_down_prop - prediction\n",
        "    error = error.reshape(batch_size, nodes, children_per_node, dim)\n",
        "    delta = einops.einsum(error, parent_down_prop, 'batch nodes children_per_node dim, batch nodes memories -> nodes children_per_node memories dim')\n",
        "    return delta.reshape(parent_mem_matrix.shape)\n",
        "\n",
        "  def outer_up(self, sensor_input):\n",
        "    outer_mm = self.layers[0]\n",
        "    h_sub_l = SparseHopfield.outer_forward_parallel(outer_mm, sensor_input, self.rho)\n",
        "    return h_sub_l\n",
        "\n",
        "  def up(self, sensor_input):\n",
        "    upwards = [self.outer_up(sensor_input)]\n",
        "    for i in range(1, len(self.layers)):\n",
        "      mm = self.layers[i]\n",
        "      h_sub_l = SparseHopfield.hidden_forward_parallel(mm, upwards[i-1], self.rho)\n",
        "      upwards.append(h_sub_l)\n",
        "    return upwards\n",
        "\n",
        "  def root_down(self, upwards, eps=1e-6):\n",
        "    root_h_sub_l = upwards[-1]\n",
        "    root_counts = self.layer_counts[-1]\n",
        "    root_h_sub_l_star, root_counts = SparseHopfield.growth_argmaxi(root_h_sub_l, root_counts, eps, self.growth_threshold)\n",
        "    self.layer_counts[-1] = root_counts\n",
        "    return root_h_sub_l_star\n",
        "\n",
        "  def down(self, upwards, eps=1e-6, coeff=0.5):\n",
        "    downwards = [self.root_down(upwards, eps)]\n",
        "    for i in range(len(upwards)-2, -1, -1):\n",
        "      h_sub_l = upwards[i]\n",
        "      counts = self.layer_counts[i]\n",
        "      downed = SparseHopfield.down_prop_parallel(downwards[-1],self.layers[i+1],h_sub_l,coeff)\n",
        "      h_sub_l_star, counts = SparseHopfield.growth_argmaxi(downed, counts, eps, self.growth_threshold)\n",
        "      self.layer_counts[i] = counts\n",
        "      downwards.append(h_sub_l_star)\n",
        "    return downwards\n",
        "\n",
        "  def pred_root_down(self, upwards, eps=1e-6):\n",
        "    root_h_sub_l = upwards[-1]\n",
        "    root_counts = self.layer_counts[-1]\n",
        "    root_h_sub_l_star = SparseHopfield.argmaxi(root_h_sub_l, eps)\n",
        "    return root_h_sub_l_star\n",
        "\n",
        "  def pred_down(self, upwards, eps=1e-6, coeff=0.5):\n",
        "    downwards = [self.pred_root_down(upwards, eps)]\n",
        "    for i in range(len(upwards)-2, -1, -1):\n",
        "      h_sub_l = upwards[i]\n",
        "      counts = self.layer_counts[i]\n",
        "      downed = SparseHopfield.down_prop_parallel(downwards[-1],self.layers[i+1],h_sub_l,coeff)\n",
        "      h_sub_l_star = SparseHopfield.argmaxi(downed, eps)\n",
        "      downwards.append(h_sub_l_star)\n",
        "    return downwards\n",
        "\n",
        "  def delta_outer(self, downwards, sensory_input):\n",
        "    outer_mm = self.layers[0]\n",
        "    outer_h_sub_l_star = downwards[-1]\n",
        "    outer_delta = SparseHopfield.mem_delta(outer_h_sub_l_star, outer_mm, sensory_input)\n",
        "    return outer_delta\n",
        "\n",
        "  def delta(self, downwards, sensory_input):\n",
        "    deltas = [self.delta_outer(downwards, sensory_input)]\n",
        "    for i in range(1, len(downwards)):\n",
        "      child_h_sub_l_star = downwards[len(downwards) - i]\n",
        "      h_sub_l_star = downwards[len(downwards) - i - 1]\n",
        "      mm = self.layers[i]\n",
        "      delta = SparseHopfield.mem_delta(h_sub_l_star, mm, child_h_sub_l_star)\n",
        "      deltas.append(delta)\n",
        "    return deltas\n",
        "\n",
        "  def optim_outer(self, deltas):\n",
        "    outer_delta = deltas[0]\n",
        "    outer_mm = self.layers[0]\n",
        "    outer_counts = self.layer_counts[0]\n",
        "    delta = outer_delta / (outer_counts).unsqueeze(-1)\n",
        "    self.layers[0] = outer_mm + delta\n",
        "\n",
        "  def optim(self, deltas):\n",
        "    self.optim_outer(deltas)\n",
        "    for i in range(1, len(deltas)):\n",
        "      delta = deltas[i]\n",
        "      mm = self.layers[i]\n",
        "      count = self.layer_counts[i]\n",
        "      delta = delta / (count).unsqueeze(-1).unsqueeze(-3)\n",
        "      self.layers[i] = mm + delta\n",
        "    self.update_iteration()\n",
        "\n",
        "  def optimize(self, sensory_input, eps=1e-6, coeff=0.5):\n",
        "    upwards = self.up(sensory_input)\n",
        "    downwards = self.down(upwards, eps, coeff)\n",
        "    deltas = self.delta(downwards, sensory_input)\n",
        "    self.optim(deltas)\n",
        "\n",
        "  def predict(self, sensory_input, eps=1e-6, coeff=0.5):\n",
        "    with torch.no_grad():\n",
        "      upwards = self.up(sensory_input)\n",
        "      downwards = self.pred_down(upwards, eps, coeff)\n",
        "      prediction = SparseHopfield.pred(downwards[-1],self.layers[0])\n",
        "      return prediction\n",
        "\n",
        "  def prediction_error(self, sensory_input, eps=1e-6, coeff=0.5):\n",
        "    with torch.no_grad():\n",
        "      pred = self.predict(sensory_input, eps, coeff)\n",
        "      return torch.mean(torch.square(pred - sensory_input))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 234,
      "metadata": {
        "id": "Lp9Se0aP9gy9"
      },
      "outputs": [],
      "source": [
        "layers: List[LayerInfo] = [\n",
        "    LayerInfo(nodes=-1, memories=15),\n",
        "    LayerInfo(nodes=3, memories=15),\n",
        "    LayerInfo(nodes=1, memories=15)\n",
        "]\n",
        "receptive_fields = 6\n",
        "field_dim = 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 235,
      "metadata": {
        "id": "CPqSlqm69kaT"
      },
      "outputs": [],
      "source": [
        "net = SparseHopfield(receptive_fields=receptive_fields,field_dim=field_dim,layers=layers, alpha=3.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 236,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(net, data):\n",
        "    batch_size, fields, dim = data.shape\n",
        "    for i in range(batch_size):\n",
        "        net.optimize(data[i].unsqueeze(0))\n",
        "\n",
        "def loss(net, data):\n",
        "    batch_size, fields, dim = data.shape\n",
        "    total_loss = 0\n",
        "    errors = []\n",
        "    for i in range(batch_size):\n",
        "        err = net.prediction_error(data[i].unsqueeze(0))\n",
        "        errors.append(err)\n",
        "        total_loss += err\n",
        "    return total_loss / batch_size, errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 237,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_tensors(tensor_dict):\n",
        "    # given a dictionary of tensors, plot them, the key will be used as the titles\n",
        "    num_tensors = len(tensor_dict)\n",
        "    fig, axes = plt.subplots(1, num_tensors, figsize=(6 * num_tensors, 6))\n",
        "\n",
        "    if num_tensors == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    for ax, (title, tensor) in zip(axes, tensor_dict.items()):\n",
        "        ax.imshow(tensor.squeeze(0))\n",
        "        ax.set_title(title)\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 238,
      "metadata": {
        "id": "x7HuJ9xRU9Mn"
      },
      "outputs": [],
      "source": [
        "batch_size = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 239,
      "metadata": {
        "id": "zzfV-UkxcZcI"
      },
      "outputs": [],
      "source": [
        "sensory_input = torch.randn(batch_size, receptive_fields, field_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 240,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor(1.1007),\n",
              " [tensor(1.1588),\n",
              "  tensor(1.3570),\n",
              "  tensor(1.2545),\n",
              "  tensor(0.9086),\n",
              "  tensor(0.8244)])"
            ]
          },
          "execution_count": 240,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loss(net, sensory_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 241,
      "metadata": {},
      "outputs": [],
      "source": [
        "train(net, sensory_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 242,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor(0.0038),\n",
              " [tensor(1.0979e-12),\n",
              "  tensor(1.3024e-12),\n",
              "  tensor(0.0095),\n",
              "  tensor(9.1325e-13),\n",
              "  tensor(0.0095)])"
            ]
          },
          "execution_count": 242,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loss(net, sensory_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 243,
      "metadata": {},
      "outputs": [],
      "source": [
        "def corruption_replace(data, noise_level=0.1):\n",
        "    # corrupts data based on noise level, each value has a `noise_level` chance of being replaced\n",
        "    mask = torch.rand_like(data) < noise_level\n",
        "    noise = torch.randn_like(data)\n",
        "    return torch.where(mask, noise, data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 244,
      "metadata": {},
      "outputs": [],
      "source": [
        "def corruption_additive(data, noise_level=0.1):\n",
        "    # corrupts data based on noise level, each value has a `noise_level` chance of being added to it\n",
        "    mask = torch.rand_like(data) < noise_level\n",
        "    noise = torch.randn_like(data)\n",
        "    return data + torch.where(mask, noise, torch.zeros_like(data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 245,
      "metadata": {},
      "outputs": [],
      "source": [
        "def corruption_revolving_lantern(data, noise_level=0.1, right_to_left=False):\n",
        "    # corrupts data based on noise level. Starting from either the right or left, it will\n",
        "    # replace the first `noise_level` percentage of the data with noise and then return\n",
        "    # the data. If `right_to_left` is True, it will start from the right, otherwise the left\n",
        "    batch, rows, cols = data.shape\n",
        "    new_data = data.clone()\n",
        "    noise_cols = int(cols * noise_level)\n",
        "    noise_cols = min(noise_cols, cols)\n",
        "    noise = torch.randn(batch, rows, noise_cols)\n",
        "    if right_to_left:\n",
        "        new_data[:, :, -noise_cols:] = noise\n",
        "    else:\n",
        "        new_data[:, :, :noise_cols] = noise\n",
        "    return new_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 246,
      "metadata": {},
      "outputs": [],
      "source": [
        "def blackouts(data, noise_level=0.1):\n",
        "    # corrupts data based on noise level. It will randomly blackout a percentage of the data\n",
        "    # based on the noise level\n",
        "    mask = torch.rand_like(data) < noise_level\n",
        "    return torch.where(mask, torch.zeros_like(data), data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 247,
      "metadata": {},
      "outputs": [],
      "source": [
        "def blackout_revolving_lantern(data, noise_level=0.1, right_to_left=False):\n",
        "    # corrupts data based on noise level. Starting from either the right or left, it will\n",
        "    # replace the first `noise_level` percentage of the data with zeros and then return\n",
        "    # the data. If `right_to_left` is True, it will start from the right, otherwise the left\n",
        "    batch, rows, cols = data.shape\n",
        "    new_data = data.clone()\n",
        "    noise_cols = int(cols * noise_level)\n",
        "    noise_cols = min(noise_cols, cols)\n",
        "    if right_to_left:\n",
        "        new_data[:, :, -noise_cols:] = torch.zeros_like(new_data[:, :, -noise_cols:])\n",
        "    else:\n",
        "        new_data[:, :, :noise_cols] = torch.zeros_like(new_data[:, :, :noise_cols])\n",
        "    return new_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
